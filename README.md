# ğŸ¤–ğŸ’¬ My Language Models from Scratch  

![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.7.1-red.svg)  

![License](https://img.shields.io/badge/license-MIT-green.svg)  
![Contributions](https://img.shields.io/badge/contributions-welcome-orange.svg)  

## ğŸ“ Project Description  
Welcome to **GPT from Scratch** ğŸ¤–ğŸ’¬ !  
The goal of this project is to **implement a Transformer model step by step**, inspired by the architecture behind **GPT (Generative Pre-trained Transformer)**.  

This repository shows how to go from **a simple Bigram model** â¡ï¸ to **a multi-layer Transformer** capable of generating text in French ğŸ‡«ğŸ‡· and English ğŸ‡¬ğŸ‡§ for example.  

---

## âš™ï¸ Features

This repository contains two different implementations of language models: a **simple Bigram model** and a **full Transformer model**.  

### ğŸ”¹ Simple Bigram Model
â†’ super simple, fast, but no context awareness  

- ğŸ—ï¸ **Architecture**: Basic bigram model using only an embedding table (`token â†’ vocab_size`)  
- ğŸ¯ **Prediction**: Each token directly predicts the next one via a lookup table  
- ğŸ“š **Dataset**: *Harry Potter* text, character-level encoding  
- âš™ï¸ **Training**: 10,000 steps with AdamW optimizer (`lr=1e-3`)  
- âš ï¸ **Limitation**: No context â€” each prediction is independent from previous ones  
- âœ¨ **Generation**: Multinomial sampling over softmax probabilities  


### ğŸ”¹ Transformer Model
â†’ powerful, context-aware, more expensive to train  

- ğŸ—ï¸ **Architecture**: Transformer with multi-head attention + feed-forward networks  
- ğŸ”‘ **Self-Attention**: Key-Query-Value mechanism with causal masking  
- ğŸ§  **Multi-Head Attention**: 6 parallel heads (`n_head=6`)  
- ğŸ“ **Positional Encoding**: Position embeddings to capture sequential order  
- ğŸ§© **Transformer Blocks**: 6 layers (`n_layer=6`) with residual connections  
- ğŸ§½ **Normalization**: LayerNorm before each sub-layer  
- ğŸ›¡ï¸ **Regularization**: Dropout (`0.2`) to reduce overfitting  
- ğŸ“ **Extended Context**: Block size of `256` tokens vs. `8` in bigram  
- ğŸ“š **Dataset**: Texts of *Victor Hugo*  
- âš¡ **Optimizations**: GPU/CUDA support, periodic train/val loss evaluation  
- ğŸ¤– **Generation**: Context-aware text generation  


## Example Outputs

After just **5,000 iterations**ğŸ‹ :
<custom-element data-json="%7B%22type%22%3A%22table-metadata%22%2C%22attributes%22%3A%7B%22title%22%3A%22Exemple%20avec%20d%C3%A9filement%22%7D%7D" />

<custom-element data-json="%7B%22type%22%3A%22table-metadata%22%2C%22attributes%22%3A%7B%22title%22%3A%22Texte%20g%C3%A9n%C3%A9r%C3%A9%20apr%C3%A8s%205%20000%20it%C3%A9rations%22%7D%7D" />

| Description        | Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|--------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Generated Text** | <div style="height: 300px; overflow-y: auto; white-space: pre-wrap;">L'homme a vie Vient pared Â»<br> Et leurs pas, Ã©branlant les arches colossales, Troublent les morts couchÃ©s sous le pavÃ© des salles.<br> Â« Oui, nous triomphons ! Venez, sÅ“urs en toutes la foules Ã©choses,<br> D'oÃ¹ fut notre prend notre tout fincens.  Le vent les dÃ©ritÃ© ! cerf, s'Ã©diffrer leur ma des voix ;<br> Et le parles mourents sourirs le profondÃ©e, Mour !<br> MÃ¨re du bois ils Dieu la vise ent l'air fait des blancs de mains croisÃ©es, Triste, tous entiÃ¨re flots que jour passe ;<br> Il pour leur verra qui son ne ferait cette dans la miÃ¨re ;<br> Le jour est tÃªte en jour, ils sont lÃ  sour ma nombre,<br> Ne velous tra. Qu'on noir mon sangla ! Pierme qu'il nous dans les femmes ?<br> Ils ne s'en vont travailler quinze heures sous dont les tiffles ; Il profond des de l'enfini qui sortes astÃ¨res,<br> La femme sous luille avec le noir poit.<br> Sans le vers main mauglant, filets attenant ; L'horreur bon est comte le vieille ; L'inge nous pare ; le maÃ®tre ; leurs mes bleaux ; S'il me vol branche l'amour, regarde, et la nuit.<br> La pauvre montagne homme a Va degrÃ©s ! Â»<br> Le vol plus Ã  cert la porte fix sont qui le partie : La maine se valle, Pour les bouche pritaint en pleint frilleur,<br> Et vous Ãªtes l'homme un flot de l'empire Ã  leur bouille !<br> Qui pourre mon cherveux qui rapportez, dans ce chacun !<br> Par Ã  peine ces deux enfants, couvres Ainsi qu'un pour toute heure ;<br> Parvu qu'il elle, frappe elle lible, Temble, Ã  dÃ©rans les coiffres qu'un regarde en tremblant son coeur,<br> Je coupens saint le vert d'enfant mennuit pleur moment.<br> Son bis non sang qu'on chevaiement plus frappÃ©.<br> Car vous Ãªtes pous l'ombre de l'amour mÃªme ! Vous Ãªtes l'oasis qu'on le luit mour conde et tout fini.<br> Oui, a regarde et de la petite flamme Son au son aeuil s'aira ces ondeul !<br> Car dans le borouche, Ã¢me en pyrÃ©che Ã  la mal !<br> Couris Ã  la la penit ses cartant pas, L'ondre effroi de sa dÃ©mon pable Ã  voix !<br> Si ma triste, S'ai je double qui pleur main et voleur !<br> Il vit, qui fui voulez : Chantez, ples mortes,  Cette foule qui fait ce que mure vous</div> |

**ğŸ” Preliminary Results:**  
With only 5,000 iterations (~4h GPU ğŸ’»ğŸ”¥), the model starts producing French-like words (though not meaningful sentences yet).

---

## âš™ï¸ How it works
- ğŸ“– Read a text dataset (Victor Hugo or Harry Potter, multilingual)
- ğŸ”¢ Create a mapping between **characters â†” integers**
- ğŸ§© Build **encoders/decoders** to switch between text and numbers
- âœ‚ï¸ Split the dataset into **training (90%)** and **validation (10%)**
- ğŸ“¦ Process text into **blocks** (context windows) and **batches**
- ğŸ—ï¸ Implement a **Bigram Language Model**
- ğŸš€ Train the model using **PyTorch** (`AdamW` optimizer)
- âœ¨ Generate new text sequences and have fun ğŸ˜† !!!

## ğŸ—ºï¸ Schema 
Here you can find the schema of a Transformer Model :
![Transformer Schema](img/Encoder-Decoder.png)

Here is a cool schema I found ! It is a realy clear explanation of the different dimension :
![Dimension Schema](img/dimension.png)

---


## ğŸ“‚ Repository structure  
```bash
â”œâ”€â”€ img/           # For the README.md
â”œâ”€â”€ text/          # Training corpora (Victor Hugo, Harry Potter, â€¦)
â”œâ”€â”€ Bigram.py      # Bigram model + first experiments  
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ Transformer.py # Full Transformer implementation  
```

---
## ğŸ’» Run it on Your PC  
Clone the repository and install dependencies:  
```bash
git clone https://github.com/Thibault-GAREL/Language_Models.git
cd Language_Models
pip install torch
# install with Cuda:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

Next, you can use Bigram Model :
```bash
python Bigram.py
```

Or use Transformer Model :
```bash
python Transformer.py
```

---

## ğŸ“– Inspiration / Sources  
This project is based on:  
- ğŸ¥ [Andrej Karpathy â€“ Let's build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY)  
- ğŸ“„ The scientific paper ["Attention is All You Need"](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need) 
- ğŸ§  OpenAIâ€™s GPT-2 / GPT-3 and [nanoGPT](https://github.com/karpathy/nanoGPT)  
